{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkred\">Motor Vehicle Collisions in New York City</span>\n",
    "### <span style=\"color:darkred\">Final Project - Explainer Notebook</span>\n",
    "---\n",
    "\n",
    "This report is written in the course 02806 Social Data Analysis, spring 2017, based on the assignment description for the final exam project found on [GitHub](https://github.com/suneman/socialdataanalysis2017/wiki/Final-Project). \n",
    "\n",
    "![Why New York City looks like it does](traffic.png)\n",
    "\n",
    "Source: [Business Insider, fetched April 2017](http://www.businessinsider.com/why-new-york-city-looks-like-it-does-2015-9?r=US&IR=T&IR=T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkred\">Motivation</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color:darkred\">What is your dataset?</span>\n",
    "\n",
    "The dataset chosen for this project is the  **\"NYC Motor Vehicle Traffic Collisions\"** dataset from the Public Safety section from the website of [New York City OpenData](http://opendata.cityofnewyork.us/). This contains traffic collision data for boroughs: Manhattan, Staten Island, Bronx, Queens and Brooklyn.\n",
    "\n",
    "Looking into where accidents happen and cross-referencing this information with the traffic density to find the areas most commonly exposed to accidents, could be used in order to *improve the safety of pedestrians* and to *take proper precautions* for the vehicles in the area whether this concerns either more mirror, traffic regulations etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color:darkred\">Why did you choose this/these particular dataset(s)?</span>\n",
    "\n",
    "We knew that we wanted to work with some sort of OpenData from a large city in the world. Going through different OpenData sources such as [London OpenData](https://data.london.gov.uk/), [SF OpenData](https://data.sfgov.org/), [European OpenData](https://open-data.europa.eu) that what seemed as a lot of different opportunities in chosing data, was in fact not. A lot of the publicly available data was lacking either in size (only a few years) or extremely badly formatted, only with a few features, a lot of missing values etc. What would be nice was if there were several years (5-10) and lots of different features to chose from. This way it would be possible to take some active choices on what to keep or discard and chose what was important for predictions, basic statistics and so on.\n",
    "\n",
    "Looking into [NYC OpenData](http://opendata.cityofnewyork.us/) and the Public Safety Category, the \"Traffic Collisions\" dataset was discovered. Here we get information on where in New York different traffic collisions happens, a timestamp for the accident, the involved parties (pedestrian, motorist biker..), whether any involved parties were killed, which type of vehicle, whether the driver was distrated/unattentive -- the contributing factors, geolocations -- a great level of detail to work on and make predictions on. More on the features of the dataset will be discussed below, first we discuss the end user's experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">What was your goal for the end user's experience?</span>\n",
    "\n",
    "<span style=\"color:red\">Should we write this more as text -- and probably focus more on the death part?</span>\n",
    "\n",
    "* Explore the data that is freely available to everyone\n",
    "* Getting to know something new about the traffic in New York\n",
    "* Learning from the patterns that can be found in the data\n",
    "* Prevent accident in the future by localizing exposed areas that need saftey improvements.\n",
    "\n",
    "From this notebook, we will try to extract some of the most informative summary staatistics and the results of our machine learning methods to try and give a better insight into the traffic in New York and when accidents happen. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Libraries</span>\n",
    "**Importing needed packages for entire solution**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named geopy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c6f76802844e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcalendar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mgeopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named geopy"
     ]
    }
   ],
   "source": [
    "#import urllib2\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import io\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\",5) # show a maximum of 10 rows in dataframe\n",
    "import geoplotlib as gpl\n",
    "from geoplotlib.utils import BoundingBox\n",
    "import calendar\n",
    "import pylab as pl\n",
    "import geopy\n",
    "import json\n",
    "from collections import Counter \n",
    "from operator import itemgetter\n",
    "from scipy import stats, linalg\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree, datasets, svm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Geopy - location\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "# Plotting with plotly\n",
    "import plotly \n",
    "from IPython.display import Image\n",
    "# Henriettes plotly API key og brugernavn -- saves plot in cloud\n",
    "plotly.tools.set_credentials_file(username='frksteenhoff2', api_key='duu8hsfRmuI5rF2EU8o5')\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "# Functions tailored for this project to minimize code in notebook \n",
    "import externalFunctions as ex\n",
    "\n",
    "# Plotting color variables\n",
    "bgBorder  = 'rgba(255, 255, 255, 0)'\n",
    "ticksAxes = 'rgb(107, 107, 107)'\n",
    "years_of_interest = [2013, 2014, 2015, 2016]\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "try:\n",
    "    to_unicode = unicode\n",
    "except NameError:\n",
    "    to_unicode = str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:darkred\">Looking at the raw data</span>\n",
    "In order to discuss the data, some of the rows from the dataset are visualized below such that it is easier to understand some of the decisions made by the group when it comes to preprocessing and cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing data using pandas\n",
    "traffic_data = pd.read_csv('Traffic_data.csv', low_memory=False, usecols=['DATE', 'TIME', 'YEAR', 'MONTH', 'HOUR', 'WEEKDAY', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2', 'ON STREET NAME', 'LOCATION', 'LONGITUDE', 'LATITUDE', 'BOROUGH', 'CONTRIBUTING FACTOR VEHICLE 1', 'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST KILLED']) # Data fetched April\n",
    "\n",
    "# Giving an example of how the data is structured (features etc.)\n",
    "traffic_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"-------------- BASIC INFORMATIOM FROM DATASET ON NYC COLLISIONS --------------\\n\" \n",
    "print \"Year range:    \", sorted(traffic_data['YEAR'].unique())\n",
    "print \"\\nDate range:    \" , min(traffic_data[\"DATE\"]), \"-\", max(traffic_data[\"DATE\"]) \n",
    "print \"\\nDataset size:  \", len(traffic_data), \"incidents\"\n",
    "print \"\\nList of areas: \", traffic_data['BOROUGH'].unique()\n",
    "print \"\\nFeatures:      \", list(traffic_data.columns), \"\\n\"\n",
    "print \"------------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkred\">The initial dataset</span>\n",
    "The original dataset has 29 columns. For ease of access to data, and to minimize computing time when working on the data, it was decided to extend the dataset with the features ``TIME, HOUR, WEEKDAY, MONTH, YEAR`` from the given timestamps of each incident. The features were calculated once and saved to the original dataset for further use. The calculations take a substantial amount of time and the group found that this was an easier solution than having to run this upon every new session.\n",
    "\n",
    "In all we have $29$ features and $1.007.310$ different incidents from July 2012 until March 24th 2017 -- the data is soo fresh! In some of our work with the data, we will be excluding year 2012 and 2017, since we only have the first and the two first Quarters 2017 and 2012, respectively. When excluded this will be stated explicitly. \n",
    "\n",
    "### <span style=\"color:darkred\">Choices in data cleaning and preprocessing</span> \n",
    "#### <span style=\"color:darkred\">Finding relevant features</span>\n",
    "Most of the features we kept and used, but we decided to removed a couple of features. Some of these features lacked any logged values and were mostly Na's and others we just didn't have the time to use fully. Such as the vehicle types and contributing factors. The variables were designed, so that bigger traffic accidents involving a lot of vehicles could be logged. But since this a rare occurence we choose not to the features. \n",
    "\n",
    "Other features we decided to expand, such as the date, so that we had an hour of the day, day of the week and month of the year variable for each observation.\n",
    "\n",
    "#### <span style=\"color:darkred\">Handling missing values</span>\n",
    "\n",
    "##### Are missing values a problem in the dataset?\n",
    "To answer the questions regarding missing values we will show some exampels from the data. We start by giving an example of a feature, ``LOCATION``, with missing values. Here we assume that a missing ``LONGITUDE`` also means a missing ``LATITUDE`` -- both are assumend calculated on the basis of the feature ``LOCATION`` and since this is the root cause of the problem, this is the feature we choose. For this example we have extracted the missing values for each month in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allData = pd.DataFrame()\n",
    "misData = pd.DataFrame()\n",
    "\n",
    "year = [2016]\n",
    "feature = \"LOCATION\"\n",
    "\n",
    "# Extract data to look at (year 2016)\n",
    "allData = traffic_data.loc[traffic_data['YEAR'].isin(year)]\n",
    "# Find all missing values in column (\"LONGITUDE\")\n",
    "\n",
    "allData[feature] = allData[feature].replace(np.NaN, 'UNKNOWN')\n",
    "misData = allData.loc[allData[feature] != 'UNKNOWN']\n",
    "\n",
    "# Print results (more or less) neatly\n",
    "print \"Accidents in all in NYC, 2016:  \", '{0:,}'.format(len(allData))\n",
    "print \"Accidents with geolocations:    \", '{0:,}'.format(len(misData))\n",
    "print \"% of accidents missing location:\", round(100.0-float(len(misData))/float(len(allData))*100, 2), \"%\\n\" \n",
    "print \"Month:    All values: Complete obs:   Missing location (%):\"\n",
    "# For each month in given year\n",
    "for numb in sorted(allData['MONTH'].unique()):\n",
    "    all_vals   = len(allData.loc[allData['MONTH'].isin([numb])])\n",
    "    nonan_vals = len(misData.loc[misData['MONTH'].isin([numb])])\n",
    "    print calendar.month_name[numb].upper().ljust(9), str('{0:,}'.format(all_vals)).ljust(10) + \"  \" + str('{0:,}'.format(nonan_vals)).ljust(12), \"   \" + str(round(100-float(nonan_vals) / float(all_vals) * 100, 2)) + \"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see that for the whole dataset, mor than $\\frac{1}{3}$ of the location values are missing! Trust us when we say that as it often is with real data, missing values are a general trend. If not, feel free to change the ``YEAR/FEATURE`` used for extracting the data and look at it yourselves.\n",
    "\n",
    "The above analysis of the missing values for location (``lon/lat``) show that the number of observations with missing values is really high. Here we are only looking into 2016, but from this alone we can see that for each month more than $14\\%$ of the values are missing. The $15\\%$ itself is not alarming, but $50\\%$ of the months, more than $\\frac{1}{4}$ of the values are missing! Especially the summer months are lacking information -- here $15\\%$ is actually what is registered correctly.   \n",
    "\n",
    "In the following sections we will comment on the missing values problem for the features relevant to our further work one by one. \n",
    "\n",
    "##### How will missing values affect our results?\n",
    "From the preliminary analysis a set of features was flagged as in need of some work in order to present the proper picture of the NYC traffic. Since we want to look into where accidents happen, why they happen it is important to have as much data as possible. Overall, what we would like is to impute the missing values with correct values where it is possible. This means that if we have the location, but we are missing the street name, we would like to compute the one based on the other. Some of our most important features with many missing values are: ``LOCATION``, ``BOROUGH``, ``ON STREET NAME`` and ``CONRIBUTING FACTORS VEHICLE 1``. These features and the considered methods for imputing the values and the final result will now be discussed. The number of missing values are calculated based on 2016 data since this will be one of te years most commonly used.\n",
    "\n",
    "##### Contributing factors\n",
    "Feature(s): ``CONTRIBUTING FACTOR VEHICLE 1``\n",
    "* Percentage of values missing: $~1\\%$\n",
    "\n",
    "Contributing factors is hard to infer from nothing. One way to work around the problem would be to impute the values with the $mode$ -- the most frequently occuring value. This would mean that the $1\\%$ of the observations missing a value would be imputed with ``DRIVER INATTENTION``. However this would falsly inflate the number of accidents with this contributing factor and since we are going to apply some machine learning tools on this feature we would want the data to eb \"as pure\" as possible. We ended up chosing to exclude the observations where contributing factor was missing, even though this might not be optimal solution.\n",
    "\n",
    "##### Location information\n",
    "Features: ``ON STREET NAME``, ``LONGITUDE/LATITUDE/LOCATION``, ``BOROUGH``\n",
    "* Percentage of values missing ``ON STREET NAME``: $~19\\%$\n",
    "* Percentage of values missing ``LON/LAT/LOC``:    $~36\\%$\n",
    "* Percentage of values missing: ``BOROUGH``:       $~32\\%$\n",
    "\n",
    "*Location (geo-location), address (street name), borough*\n",
    "\n",
    "All of these columns concern location just on different levels. Coordinates which is the most precise, street name that describe a larger, still well-defined area, and boroughs that look at a larger geographically defined area. As can be seen from the percentage given above, in all of the columns, there are several missing values.  \n",
    "\n",
    "The reason why we are describing these features together is because they all can be imputed using the same python module, namely ``geopy``. The module matches, ``lon/lat``-pairs to find a street name or street names and city to ``lon/lat``-pairs, which is exactly what we need! Below we have tried to map the number of observations missing the different featured mentioned and give an example of how ``geopy`` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replce missing borough values with \"unspecified\"\n",
    "traffic_data['BOROUGH']        = traffic_data['BOROUGH'].replace(np.NaN, 'Unspecified')\n",
    "traffic_data['LOCATION']       = traffic_data['LOCATION'].replace(np.NaN, 'Unspecified')\n",
    "traffic_data['ON STREET NAME'] = traffic_data['ON STREET NAME'].replace(np.NaN, 'Unspecified')\n",
    "\n",
    "# Extract the unspecified values\n",
    "missing_bor = traffic_data.loc[traffic_data['BOROUGH']        == 'Unspecified']\n",
    "missing_loc = traffic_data.loc[traffic_data['LOCATION']       == 'Unspecified']\n",
    "missing_st  = traffic_data.loc[traffic_data['ON STREET NAME'] == 'Unspecified']\n",
    "\n",
    "# Number of missing values for each feature in dataset \n",
    "len(missing_bor), len(missing_loc), len(missing_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all observations missing borough, location AND street name\n",
    "missing_bl = missing_bor.loc[missing_bor['LOCATION'] == 'Unspecified']\n",
    "all_3      = missing_bl.loc[missing_bl['ON STREET NAME'] == 'Unspecified']\n",
    "len(all_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost $50,000$ rows are missing both ``location``, ``on street name`` and ``borough``. This is not that much considering that we are working $20$ times as much data.\n",
    "\n",
    "##### ``geopy`` and how it works\n",
    "Let's give an example, we have the coordinates, and want street name/borough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location = geolocator.reverse(\"40.762737, -73.83951\")\n",
    "location.address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, by giving the geolocation for a feature with a missing street address, we can extract the street name from the ``location`` object with the call to ``.address``. If we want to go the other way (finding geolocations from street name) we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location = geolocator.geocode(\"Van Wyck Expressway, New York City\")\n",
    "location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, from geolocation we get a string where we can extract an address and the borough. By giving the street name and city, which will always be New York, we get the geolocations, which is exactly what we want.\n",
    "\n",
    "However. We have run in to one major problem. The number of times you can call the ``geopy`` API per day is not enough for our purposes, as we have to make an excessive amount of calls, even if we extract both address and borough in the same call. Therefore, we have not been able to impute all our values correctly. Therefore, for the most of our data analysis, missing values have been excluded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkred\">Summary statistics and basic plotting</span>\n",
    "### <span style=\"color:darkred\">Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)</span>\n",
    "\n",
    "To get an idea of the traffic situation in New York, what types of incidents, their outcome, what is the most common places for accidents to happen etc. we have done some different plotting of all collisions historically, per year, per borough and for specific vehicle types, times of day etc. Thus we hope to give you a better idea of the traffic scene in NYC.\n",
    "\n",
    "The plots will start by describing basic summary statistics but will increase in complexity throughout the notebook. The plots will all be rendered using ``plotly`` and it's API in order to minimize the length of the notebook by exhibiting more information as mouse-over visuals directly in the plot itself.\n",
    "\n",
    "The description associated with each plot will be positioned underneath the plots such that the reader can get a chance to make his/her own thoughts about the data before we bring our comments.\n",
    "\n",
    "### <span style=\"color:darkred\"> Inflicted boroughs</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count incidents in each borough\n",
    "borough_cnt = ex.countSamples(traffic_data['BOROUGH'].replace(np.NaN, 'Unspecified'))\n",
    "\n",
    "# Plot them\n",
    "ex.createBarPlot(borough_cnt, 'Collisions per borough in all', 'Borough', 'Number of collisions', 'collisions_all', bgBorder, ticksAxes, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot it is easy to see that we are working with a lot of missing data (seen in bar \"Unspecified\"). If we however, overlook this for a while it looks as if Brooklyn is the borough where most accidents happen. Staten Island is by far the least inflicted borough, knowing it's geographical location and position relative to Manhattan/Brooklyn this makes good sense. Queens and Manhattan seem to have approximately the same number of accidents. \n",
    "\n",
    "We will look into whether this picture portrays the reality correct later with some geoplotting with ``geoplotlib``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Plotting the accidents</span>\n",
    "Since we have a lot of missing information in the dataset, we chose to check whether the number of accidents would look the same plotted on a density map as it does when making the bar chart. \n",
    "\n",
    "Becuase of the high number of accidents each year, we have chosen only to plot a single month (May 2016) to give an idea of how "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year = 2016\n",
    "yearData = traffic_data.loc[traffic_data['YEAR'].isin([year])]\n",
    "oneMonth = yearData.loc[yearData['MONTH'].isin([5])] \n",
    "# Plot inline \n",
    "ex.plotGeoData(oneMonth, 'MONTH', \"Current month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking at the heat map of the accidents, the picture does not show that it is in fact Brooklyn that has the highest amount of accidents. If we look at Manhattan, the number of accidents is clearly higher here. This plot can make up for some of the missing values that are found for the boroughs and shows that Manhattan is actually \"a bigger sinner\" than both Queens and Brooklyn when it comes to accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving geoplot images to use on webpage\n",
    "year = 2012\n",
    "yearData = traffic_data.loc[traffic_data['YEAR'].isin([year])]\n",
    "#ex.saveGeoData(yearData.loc[yearData['CONTRIBUTING FACTOR VEHICLE 1'].isin(['Driver Inattention/Distraction'])], 'MONTH', \"Current month\", str(year)[2:])\n",
    "ex.saveGeoData(yearData, 'MONTH', \"Current month\", str(year)[2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Primary vehicle in collision - per vehicle type</span>\n",
    "In order to look into which types of vehicles that are most commonly involved in accidents, we have plottet all the vehicles involved in accidents for all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace NaN with 'unknown'\n",
    "vehicle_types = traffic_data['VEHICLE TYPE CODE 1'].replace(np.NaN, 'OTHER')\n",
    "\n",
    "# Count key/value pairs\n",
    "collision_count = ex.countSamples(vehicle_types)\n",
    "\n",
    "# createBarPlot(valueDict, plotTitle, xtitle, ytitle, saveName, bgBorder, ticksAxes, marginValue):\n",
    "vehicles = ex.createBarPlot(collision_count, 'Primary vehicle in collison, NY 2012-2017', 'Vehicle type', 'Number of incidents', 'vechicle2', bgBorder, ticksAxes, 200)\n",
    "vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passenger vehicles and sport/utility wagons are by far the two vehicle types that are most commonly involved in accidents. Out of the total of $1.007.130$ accidents, passenger vehicles are involved in more than $50\\%$ of the accidents while the sport/utility wagon only account less than half of that. This visualization is also added on the webpage and will be described in further detail there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save as json object for website\n",
    "with open('primary_vehicle.json', 'w') as fp:\n",
    "    json.dump(collision_count.most_common(), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Vehicle collisions - per contributing factor</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Replace NaN with 'Unspecified'\n",
    "factor_types = traffic_data['CONTRIBUTING FACTOR VEHICLE 1'].replace(np.NaN, 'Unspecified')\n",
    "\n",
    "# Count number of occurences of the different categories\n",
    "factor_count = ex.countSamples(factor_types)\n",
    "\n",
    "# Remove unspecified in order to get a better picture of remaining entries\n",
    "del factor_count['Unspecified']\n",
    "\n",
    "# Save as json object for website\n",
    "with open('factor_data.json', 'w') as fp:\n",
    "    json.dump(factor_count.most_common(25), fp)\n",
    "\n",
    "# createBarPlot(valueDict, plotTitle, xtitle, ytitle, saveName, bgBorder, ticksAxes, marginValue):\n",
    "factors = ex.createBarPlot(factor_count, 'Contributing factors for collisions, NY 2012-2017', 'Reason/factor', 'Number of incidents', 'factors', bgBorder, ticksAxes, 250)\n",
    "factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the primary vehicle in collison and the contributiog factors are included as visualization on the final webpage. These two summary statistics show some of the basic facts about the different traffic accidents in New York and gives us a chance to show how to work with switching between different datasets in the same visualization, how to create the initial bar chart and update it with completely new values for both axes. We think this is a good way to show that we have understood the basic functionalities within the d3 visualization techniques when creating bar chart while it also gives the reader some intial information about the traffic in NYC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\"> Plotting the collisions per year</span>\n",
    "Here, only data in range $2013-2016$ is used as the rest is incomple and will distort the overall picture as the data is plotted per year. in all, 10 vehicle typs are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create 2 X 5 grid for the 10 collision subplots\n",
    "# Initialize variables\n",
    "vehicle_types = ['PASSENGER VEHICLE', 'SPORT UTILITY / STATION WAGON',\n",
    "                'VAN','BUS','SMALL COM VEH(4 TIRES)', 'LIVERY VEHICLE', 'MOTORCYCLE',\n",
    "                 'PICK-UP TRUCK', 'LARGE COM VEH(6 OR MORE TIRES)', 'TAXI']\n",
    "y_cnt = Counter()\n",
    "i = 1 # plotting variable keeping track of column\n",
    "k = 1 # plotting variable keeping track of row\n",
    "j = 1 # plotting variable keeping track of axis number\n",
    "\n",
    "fig = tools.make_subplots(rows=5, cols=2, subplot_titles=(vehicle_types))\n",
    "\n",
    "# Run through each vehicle type\n",
    "for vehicle in vehicle_types:\n",
    "    # Make data structure with one collision category at a time\n",
    "    temp_values = traffic_data.loc[traffic_data['VEHICLE TYPE CODE 1'].isin([vehicle])]\n",
    "    # Add column with year\n",
    "    for year in [2013, 2014, 2015, 2016]:\n",
    "            y_cnt[year] = len(temp_values.loc[temp_values['YEAR'].isin([year])])\n",
    "    \n",
    "    trace = go.Bar(\n",
    "    x=y_cnt.keys(),\n",
    "    y=y_cnt.values()\n",
    "    )\n",
    "    fig.append_trace(trace, k, i)\n",
    "    fig['layout']['yaxis'+str(j)].update(title='Number of incidents', range=[0, max(y_cnt.values())])\n",
    "    \n",
    "    if(i < 2):\n",
    "        i += 1\n",
    "    else:\n",
    "        i = 1\n",
    "        k += 1\n",
    "    j += 1\n",
    "    \n",
    "fig['layout'].update(height=1200, width=1000, showlegend=False, title=\"Accidents per vehicle type 2013-2016\", margin=dict(b=200))\n",
    "py.iplot(fig, filename='subplot_vehicles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As in our previous assignments we have plotted the number of accidents for each year for some of the most common vehicle types involved in accidents. There are several patterns in the vehicle specific development of accidents from $2013-2016$. It looks as if the larger vehicles have had a decrease in number of accidents in 2016 compared to earlier years, whereas the number of accidents for passenger vehicles and motor cycles have increased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\"> Number of traffic deaths</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "persons_killed = traffic_data['NUMBER OF PERSONS KILLED'].replace(np.NaN, 'OTHER')\n",
    "kill_count = {}\n",
    "for val in traffic_data['YEAR'].unique():\n",
    "    if val != np.int64(2017) and val != np.int64(2012):\n",
    "        temp = traffic_data.loc[traffic_data['YEAR'].isin([val])]\n",
    "        kill_count[val] = int(temp['NUMBER OF PERSONS KILLED'].sum(axis=0))\n",
    "\n",
    "#killed = ex.createBarPlot(kill_count, 'Number of persons killed per year, 2012-2017', 'Year', 'Number of deaths', 'deaths_all', bgBorder, ticksAxes, 150)\n",
    "#killed\n",
    "killed = ex.createXYBarPlot(kill_count.keys(), kill_count.values(), 'Number of persons killed per year in NYC, 2012-2017', 'Year', 'Number of deaths', 'deaths_all', bgBorder, ticksAxes, 150)\n",
    "killed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Involved parties</span>\n",
    "* [Subplot documentation for Plotly](https://plot.ly/python/subplots/)\n",
    "\n",
    "#### A slowly decreasing trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracting needed information\n",
    "focus_group = ['NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST KILLED']\n",
    "focus = {}\n",
    "\n",
    "# Creating initial figure for subplots\n",
    "fig = tools.make_subplots(rows=1, cols=4, subplot_titles=(\"2013\", \"2014\", \"2015\", \"2016\"))\n",
    "\n",
    "i = 1 # Keeping track of plot grid columns\n",
    "k = 1 # Setting axis range for all y axes \n",
    "    \n",
    "for year in years_of_interest:\n",
    "    for group in focus_group:\n",
    "        yearly_data = traffic_data.loc[traffic_data['YEAR'].isin([year])]\n",
    "        focus[group] = yearly_data[group].sum()\n",
    "    \n",
    "    trace = go.Bar(\n",
    "    x=focus.keys(),\n",
    "    y=focus.values(),\n",
    "    name=year\n",
    "    )\n",
    "    # Add plot to figure\n",
    "    fig.append_trace(trace, 1, i)\n",
    "    fig['layout']['yaxis'+str(k)].update(title='Year', range=[0, 180])\n",
    "    \n",
    "    i += 1 \n",
    "    k += 1\n",
    "\n",
    "# Plot result\n",
    "fig['layout'].update(height=600, width=1100, showlegend=False, title='Casualties per year in NYC traffic accidents', margin=dict(b=200))\n",
    "py.image.save_as(fig, filename='subplots-deaths.png')\n",
    "py.iplot(fig, filename='simple-subplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of people killed in traffic accidents in New York is slowly decreasing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Are the incidents prone to happen at a specific time of day?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bor = [\"Unknown\", 'BROOKLYN', 'BRONX', 'MANHATTAN', 'QUEENS', 'STATEN ISLAND']\n",
    "n = ex.plotBarInSubplotGrid(2, 3, bor, \"Incidents in NYC per hour\", traffic_data, 'BOROUGH', 'HOUR', 'hourly', 21000, 1000, 1200)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving number of accidents per borough per year to json object for website\n",
    "hour_dict = {}\n",
    "for borough in bor:\n",
    "    wrk = traffic_data.loc[traffic_data['BOROUGH'].isin([borough])]\n",
    "    # Count incidents in each borough\n",
    "    borough_cnt = countSamples(wrk['HOUR'])\n",
    "    hour_dict[borough] = dict(borough_cnt.most_common())\n",
    "\n",
    "# Save data structure to json object\n",
    "with open('hour_data.json', 'w') as fp:\n",
    "    json.dump(hour_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "----\n",
    "## <span style=\"color:darkred\">The Demographics of New York</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demographics of New York \n",
    "ny_demographics = pd.read_excel('NY_demographics.xlsx')\n",
    "ny_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Demographics of new York - Wikipedia](https://en.wikipedia.org/wiki/Demographics_of_New_York_City)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find number of people per square kilometer for each borough in NYC\n",
    "i = 0\n",
    "for values in ['BRONX', 'BROOKLYN', 'MANHATTAN', 'QUEENS', 'STATEN ISLAND']:\n",
    "    print values, float(borough_cnt['BRONX'])/ny_demographics.iloc[i,1]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot person per square kilometer\n",
    "ppl_pr_sqkm = ex.createXYBarPlot(ny_demographics['borough'], ny_demographics['personsPrSqKm'], \"Number of people per square kilometer\", 'Borough', 'Number of people', 'ppl_pr_sqkm', bgBorder, ticksAxes, 100)\n",
    "ppl_pr_sqkm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot it is made clear that indeed Manhattan has a great deal of people per square kilometer. **Manhattan has in fact more than double the amount of people compared any of the neighbouring boroughs**. Naturally one should expect more incidents here, since more people makes for a more activity both on the sidewalks and in the streets. One thing one also have to take into account is the high number of people visitng Manhattan which without a doubt will make the number of people per square kilometer even higher.\n",
    "\n",
    "The demographics of NYC was included as we had hoped this would be able to give some more insight into whether some accidents happened in less populated places or not. As the population of an area and the number of accidents seem to be somewhat correlated (many people, many incidents) we did not use this data for further work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## <span style=\"color:darkred\">Theory - theoretical tools</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Machine learning tools and why tools chosen are right for our problem.</span>\n",
    "* **K-means**: We chose to cluster our data using K-means, as we wanted to know how the distribution of traffic accidents in NYC changed over the years. Since K-means is an unsupervised machine learning method it is a good choice to use when exploring data. Thus we decided not use any target data for the model. We chose data from may in 2013-2016 as the whole dataset contained too many observations for the clustering to run smoothly on the website.\n",
    "* **Decision tree**: We chose to use decision trees to predict the causes behind the accidents. We wanted to use more than just geolocation to predict on our data and a decision tree seemed fitting as it can use both categorical aswell as continous features at the same time. Furthermore it is also tool which can be easily visualized and understood by the reader. Decision trees can be used to find hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Model selection and splitting of data</span>\n",
    "* **K-means**: For clustering we didn't need to split the data into test and training sets, since we opted to model the data without targets. This decision also means that we didn't need (and couldn't use) cross validation to test our model preformance.\n",
    "* **Decision tree**: To later be able to test the preformance of the decision tree classifier we decided to split the data into a training and testing set. This split consited accordingly of 90 % and 10 % of the data with the use of the function train_test_split from the scikitlearn library. For the decision tree we decided to reduce the number of targets from the original 49 to the 10 most common factors. In regards to the features wetried using street name as a variable, but this decision resulted in gross overfitting. But why? The answer is simple. The feature of street names contain too many different categories as there of course are a lot of different streets in NYC. The model would make too many very specific rulesto fit the training data perfectly. Thus we decided to drop the feature from the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color:darkred\">Model performance</span>\n",
    "* **K-means**: The lack of target data again means that model performance can't be done.\n",
    "* **Decision tree**: From splitting the dataset into training and test data we are able to measure preformance on the model on both the data subsets. This way we were able to see if our model overfitted to the training data. The preformance measure we used was just to see how big a percent of the datasets we predicted correctly. The final model we produced had a training and testing error of around 35.5 % which is not great but still better than random, which would be around 10 % since we have 10 different targets. It would also have been nice to be able to use sofisticated pruning techniques on the model but they aren't supported by scikit-learn, so we chose to just restrict the maximum depth of the tree to 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkred\">Solutions for the machine learning methods</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">K-means clustering</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Importing data set (used for both machine learning methods)\n",
    "filename = 'Traffic_data.csv'\n",
    "traffic_data = pd.read_csv(filename, low_memory=False)\n",
    "traffic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extracting coordinate data\n",
    "coordinates  = traffic_data[['LATITUDE','LONGITUDE','YEAR']].dropna(axis = 0, how = 'any')\n",
    "focus_years  = [2013,2014,2015,2016]\n",
    "\n",
    "# Number of clusters to use\n",
    "n_clusters   = 4\n",
    "\n",
    "# Initializing dictionaries to store data\n",
    "cluster_data = {}\n",
    "centroids    = {}\n",
    "\n",
    "for year in focus_years:\n",
    "    data = coordinates.loc[coordinates['YEAR'].isin([year])]\n",
    "    \n",
    "    # Train KMeans on data\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(data[['LATITUDE','LONGITUDE']])\n",
    "    \n",
    "    # Save the centroids to an array\n",
    "    centroid = kmeans.cluster_centers_\n",
    "    centroid_array = []\n",
    "    \n",
    "    for j in range(0,n_clusters):\n",
    "        centroid_cord = {}\n",
    "        centroid_cord['LATITUDE'] = centroid[j][0]\n",
    "        centroid_cord['LONGITUDE'] = centroid[j][1]\n",
    "        centroid_array.append(centroid_cord)\n",
    "    centroids[str(year)] = centroid_array\n",
    "    \n",
    "    data['CLUSTER'] = kmeans.predict(data[['LATITUDE','LONGITUDE']])\n",
    "    \n",
    "    cluster_data[str(year)] = data[['LATITUDE','LONGITUDE','CLUSTER']].to_dict('list')\n",
    "\n",
    "model_data = {}\n",
    "model_data['centroids'] = centroids\n",
    "model_data['datapoints'] = cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write JSON file with datapoints\n",
    "with io.open('model_data.json', 'w', encoding='utf8') as outfile:\n",
    "    str_ = json.dumps(model_data,\n",
    "                      indent=4, sort_keys=True,\n",
    "                      separators=(',', ':'), ensure_ascii=False)\n",
    "    outfile.write(to_unicode(str_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:darkred\">Decision Tree</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicating data \n",
    "features  = ['CONTRIBUTING FACTOR VEHICLE 1','BOROUGH','NUMBER OF PERSONS KILLED','VEHICLE TYPE CODE 1',\n",
    "            'HOUR','WEEKDAY']\n",
    "tree_data = traffic_data[features]\n",
    "\n",
    "# Replacing NA's with 'OTHER' for vehicle type\n",
    "tree_data['VEHICLE TYPE CODE 1'].fillna('OTHER', inplace=True)\n",
    "\n",
    "tree_data = tree_data[tree_data['CONTRIBUTING FACTOR VEHICLE 1'] != 'Unspecified']\n",
    "tree_data = tree_data[tree_data['BOROUGH'] != 'UNKNOWN']\n",
    "tree_data = tree_data.dropna(axis = 0, how = 'any')\n",
    "\n",
    "# Remove uncommon contributing factors (as shown in plot of factors)\n",
    "common_factors = ['Driver Inattention/Distraction','Fatigued/Drowsy','Failure to Yield Right-of-Way',\n",
    "                  'Other Vehicular','Backing Unsafely','Turning Improperly','Lost Consciousness','Prescription Medication',\n",
    "                  'Traffic Control Disregarded','Driver Inexperience']\n",
    "\n",
    "tree_data = tree_data.loc[tree_data['CONTRIBUTING FACTOR VEHICLE 1'].isin(common_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tree_data[features[1:]]\n",
    "# Encode input values as an enumerated type or categorical variable\n",
    "X.loc[:,'BOROUGH']                  = pd.factorize(X['BOROUGH'])[0]\n",
    "#X.loc[:,'ON STREET NAME']          = pd.factorize(X['ON STREET NAME'])[0]\n",
    "X.loc[:,'NUMBER OF PERSONS KILLED'] = pd.factorize(X['NUMBER OF PERSONS KILLED'])[0]\n",
    "X.loc[:,'VEHICLE TYPE CODE 1']      = pd.factorize(X['VEHICLE TYPE CODE 1'])[0]\n",
    "X.loc[:,'HOUR']                     = pd.factorize(X['HOUR'])[0]\n",
    "X.loc[:,'WEEKDAY']                  = pd.factorize(X['WEEKDAY'])[0]\n",
    "\n",
    "y = pd.factorize(tree_data[features[0]])[0]\n",
    "\n",
    "# Splitting testing and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "# Function for json structure for tree visualization\n",
    "def rules(clf, features, labels, node_index=0):\n",
    "    \"\"\"Structure of rules in a fit decision tree classifier\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : DecisionTreeClassifier\n",
    "        A tree that has already been fit.\n",
    "\n",
    "    features, labels : lists of str\n",
    "        The names of the features and labels, respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    node = {}\n",
    "    if clf.tree_.children_left[node_index] == -1:  # indicates leaf\n",
    "        count_labels = zip(clf.tree_.value[node_index, 0], labels)\n",
    "        node['name'] = ', '.join(('{} of {}'.format(int(count), label)\n",
    "                                  for count, label in count_labels))\n",
    "    else:\n",
    "        feature = features[clf.tree_.feature[node_index]]\n",
    "        threshold = clf.tree_.threshold[node_index]\n",
    "        node['name'] = '{} > {}'.format(feature, threshold)\n",
    "        left_index = clf.tree_.children_left[node_index]\n",
    "        right_index = clf.tree_.children_right[node_index]\n",
    "        node['children'] = [rules(clf, features, labels, right_index),\n",
    "                            rules(clf, features, labels, left_index)]\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model training and prediction\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "test_pred = clf.predict(X_test)\n",
    "train_pred = clf.predict(X_train)\n",
    "\n",
    "print 'The test score is:    ', ex.score(test_pred,y_test)\n",
    "print 'The training score is:', ex.score(train_pred,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = rules(clf, features[1:], common_factors)\n",
    "with open('rules.json', 'w') as f:\n",
    "    f.write(json.dumps(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## <span style=\"color:darkred\">Visualizations</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This text is written assuming that the visualizations for discussion are the ones on the webpage. The rest should highly resemble some of the visualizations made for the other assignments in this course and are described briefly in the Notebook, but are assumed less important to describe as these were \"mandatory\".*\n",
    "\n",
    "In order to give an introduction as to what information that could be found in the dataset, we chose to start out by visualizing some basics such as vehicles most commonly involved in collisions and what factors contribute to accidents. This information is also something that we will use in our machine learning models and therefore we though it relevant to include. By letting a button click switch between the two datasets, we could use this visualization to show that we also understand how to change axes and data in a plot, add tooltips and bind elements to events using ``d3`` and basic JavaScript/HTML. Here, we do not use any transistions and the tooltip is different from some of the later bar charts. \n",
    "\n",
    "The geoplots of all the accidents per month in our dataset was added in order to give the reader a picture of where the accidents happen. By plotting them om the map it is much easier to get an idea of both the volume of the accidents and the veracity. As there are many months to go through, the two bar plots of fatal accidents and all accidents per year were added to support and summarize some of the information hidden in the geoplots. \n",
    "\n",
    "One of the plots digging a little deeper into the differences between the accidents in each borough is the bar chart of accidents per hour in each borough. Here it gets really easy to compare the different boroughs and look for patterns. Further more, this plot really takes advantage of the great visualizations techniques that ``d3`` has to offer which can be seen in the transistion between the different boroughs.\n",
    "\n",
    "When have decided upon using k-means, clustering the data onto the NYC shapefile was the only way we wanted to visualize the data. We are aware of the fact that this also has been done in one of the previous assignments, but since we are no longer working with the same geographic location we have had to find the shapefile ourselves and apply a clustering that we felt suitable. That is also why we chose to use only one cluster size and instead look at the different years. This way we could use the clustering to say something about the accidents in general i.e do the clustering change a lot from year to year and why might this be.\n",
    "\n",
    "When trying to predict the causes behind the accidents, we decided upon the decision tree. By applying this visualization to the result we tried to show that we also are able to try visualizations that we did not learn in the course. This way we feel that we both cover the basics that we should know but also go a bit further by trying something new and letting the reader explore the data as he/she clicks herself through the tree.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## <span style=\"color:darkred\">Discussion - critical thinking</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color:darkred\">What went well?</span>\n",
    "* Model selection and prediction went farily well all in all. Even though the predictive power of the decision tree wasn't too strong.\n",
    "* Set-up/design of webpage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color:darkred\">What is still missing? What could be improved? Why?</span>\n",
    "#### <span style=\"color:darkred\">Data-wise</span>\n",
    "\n",
    "##### Missing values\n",
    "We really wanted to do some more work on the missing values in the dataset. We found the module ``geopy`` which lets you write an address and get corresponding geo location or the other way around, write geo location and get street information such as Street name, number, state, country etc. an example of both is given below.\n",
    "\n",
    "The problem however is that the API that is called to get the geo locations/address values only allow a minimum of calls per day and with a total of $32.396$ values for just location, this has been impossible to acheive. However we still wanted to mention it as this was our intention and probably would have been easy to do had we had a developer account. \n",
    "\n",
    "##### Combining data\n",
    "It would have been great if we could have combined out data with some information about traffic densities at specific times of day, this way we could have learned more about whether the traffic density in any way affects the number of accidents. Another cool thing to look into could have been the weather for each day at the given location. We did not however try to find or extend our dataset with this information.\n",
    "\n",
    "In general adding features would have made it possible to say something new about the data that is not limited to summary statistics but makes the end result vaulable and possibly finding some new and useful information.\n",
    "\n",
    "##### Data size\n",
    "Some of our data structures (the ``.json``-shapefile takes up a lot of space. It would have been great if we could have compressed this, but this has not been done. As the different clusters that depend on the shapefile was rendered pretty quickly this was seen as suffifienct, however it is definitely a point for improvement.\n",
    "\n",
    "Regarding the shapefile, it could also have been good had we had some indentification of the different borough boundaries. We were unable to find susch a file and had to work with what we could find. However, in our last assignment, there were some complaints about not being able to see the different names on the shapefile due to a lot of observations on top and this problem would without a doubt also be a problem this time.\n",
    "\n",
    "#### <span style=\"color:darkred\">Machine Learning</span>\n",
    "Had we extended our dataset with some new features, we could have made some different, possibly much more interesting predictions. If we had some infromation on traffic density we could the probability of traffic at a specific location being highly or less trafficked and if we included weather we could have predicted the whether an accident at a given day/time at a specific location would be fatal etc. \n",
    "\n",
    "For the models that we chose to work with, there were a couple of things we could have done differently. With the decision tree we could have tried to use an ensemble method such as Random Forrests, to see if that could have improved our final model. Also pruning would also have been nice, but since it isn't offered by scikit-learn we would have had to either implement it ourselves or find another machine learning library that supported it. Had we also tried to predict while clustering we might have been able to find some interresting patterns. But from the visualization we concluded that the data was mostly not very easily clustered. \n",
    "\n",
    "We could also have tried using some other machine learning tools, as they might have preformed better than what we ended up using. This is of course difficult to say and some methods, like neural networks would probably require a lot more data preprocessing or maybe even some external data. \n",
    "\n",
    "#### <span style=\"color:darkred\"> Visualizations</span>\n",
    "We have been looking at the data at a very high level throughout the entire report. If we instead had decided upon a specific year and looked into specific accidents over time, per day etc. (possibly something like this from [ArcGIS](https://www.arcgis.com/home/webmap/viewer.html?webmap=df41300d3d4344baa080c86349bfd59b) we might have found something more interesting even though it might not be something one could generalize for all accidents.\n",
    "\n",
    "We could have used a broader variety of visualizations. On the webpage, there are a lot of bar charts. Maybe we could have scaled down on these and used a different plotting type, but since we have already used scatter plots and since pie-charts are only good for showing how much of a pie one has actually eaten we did not use any other plotting type. To \"spice it up\" we could have made use of grouped bar charts in order to plot either several years for all the boroughs in the same plot -- this on the other hand would have made transitions obsolete, and we wouldn't want to miss out on those as they have been such a large part of the course.\n",
    "\n",
    "We wanted to make the webpage scrollable (having the content move to the middle of the page such that everything felt as if it was happening in a continouous stream. This was one of the things that we did not reach to look more into. The basic idea however, was something like this Mike Bostock explains it here [How to Scroll](https://bost.ocks.org/mike/scroll/), and is visualized with examples here: [scroll.js](http://1wheel.github.io/graph-scroll/).\n",
    "\n",
    "Visualizing the number of accidents per month for all our dat might ba a little much to take in and hard to get something out of since there are so many incidents. This could have been improved if we, again, had decided on working with data on a smaller scale say, only observations where the contributing factor was driver inattention in 2016. However, this taught us how to add images as a series to a webpage and include text on the different images. \n",
    "\n",
    "#### <span style=\"color:darkred\"> Overall in the solution</span>\n",
    "We have processed and understood large amount of data from a selected source. Using tools for working with data from this course, we have succesfully visualized different summary statistics and machine learning methods/results using d3. We have found some patterns and commented on why this might be.\n",
    "\n",
    "We have successfully tested, trained and visualized two different machine learning models and applied cross validation in order to obtain the best fit possible. For the sake of the visualization, we have had to prune the decision tree quite extensively, by setting the maximum depth of the tree to 4, as it otherwise would have been hard to fit onto the webpage. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkred\">References</span>\n",
    "In relation to traffic in New York City, we have found a few sources describing the traffic in New York and some previous visualizations:\n",
    "#### <span style=\"color:darkred\">Related articles and inspiration</span>\n",
    "\n",
    "**For visualizing the decision tree:**\n",
    "\n",
    "* Aaron Schumacher, *See sklearn decision trees with d3*, [http:///planspace.org](http://planspace.org/20151129-see_sklearn_trees_with_d3/)\n",
    "\n",
    "**Shapefile used for plotting the k-means clustering:**\n",
    "* New York City Government, Department of City Planning, *Borough Boundaries and Community Districts* [http://www1.nyc.gov](http://www1.nyc.gov/site/planning/data-maps/open-data/districts-download-metadata.page)\n",
    "\n",
    "**Switching between images on webpage**\n",
    "* StackOverflow, [Switching between images](http://stackoverflow.com/questions/13975891/change-image-in-html-page-every-few-seconds)\n",
    "\n",
    "**Inspiration and previous visualization of New York:**\n",
    "* [**Viewing.nyc's**](https://viewing.nyc/)\n",
    "  * Matt Coneybear, *See Wazes Hypnotic Visualization of NYC traffic Patterns* [http//:viewing.nyc](https://viewing.nyc/see-wazes-hypnotic-visualization-of-new-york-city-traffic-patterns/)\n",
    "  * Matt Coneybear, *Here's a scary map of all distracted driving collisions in New York City* [http://viewing.nic](https://viewing.nyc/heres-a-scary-map-of-all-distracted-driving-collisions-in-new-york-city/)\n",
    "* The New York Times,  *Number of traffic deaths in New York falls for a second year in a row* [https://nytimes.com](https://www.nytimes.com/2016/01/02/nyregion/number-of-traffic-deaths-in-new-york-falls-for-a-second-year-in-a-row.html?_r=0)\n",
    "\n",
    "**Basic Vision Zero information:**\n",
    "* New York City Government, *Vision Zero*, [www1.nyc.gov](www1.nyc.gov/visionzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
